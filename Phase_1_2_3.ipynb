{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0c3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import argparse\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from collections import deque\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b08187",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityWebScraper:\n",
    "    def __init__(self, start_urls, output_dir, max_pages=1000, delay=1, concurrent=5):\n",
    "        self.start_urls = start_urls\n",
    "        self.output_dir = output_dir\n",
    "        self.max_pages = max_pages\n",
    "        self.delay = delay\n",
    "        self.concurrent = concurrent\n",
    "        \n",
    "        # Track visited URLs to avoid duplicates\n",
    "        self.visited = set()\n",
    "        \n",
    "        # URLs to be visited\n",
    "        self.url_queue = deque(start_urls)\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        # Set up logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(os.path.join(output_dir, 'scraper.log')),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Headers to simulate a real browser\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Referer': 'https://www.google.com/',\n",
    "            'DNT': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        }\n",
    "        \n",
    "        # Regular expressions for finding Unity-related content\n",
    "        self.unity_patterns = [\n",
    "            r'unity', r'game.*develop', r'component', r'gameobject',\n",
    "            r'transform', r'scripting', r'c#', r'shader', r'material',\n",
    "            r'animation', r'physics', r'collision', r'rigidbody',\n",
    "            r'prefab', r'scene', r'editor', r'UI', r'input', r'camera'\n",
    "        ]\n",
    "        \n",
    "        # Domains that are allowed for scraping\n",
    "        self.allowed_domains = [\n",
    "            'unity.com',\n",
    "            'docs.unity3d.com',\n",
    "            'learn.unity.com',\n",
    "            'unity3d.com',\n",
    "            'forum.unity.com',\n",
    "            'answers.unity.com',\n",
    "            'gamedev.stackexchange.com',\n",
    "            'unitycodemonkey.com',\n",
    "            'catlikecoding.com',\n",
    "            'raywenderlich.com',\n",
    "            'brackeys.com',\n",
    "            'unity3d.college',\n",
    "        ]\n",
    "        \n",
    "        # Counter for scraped pages\n",
    "        self.pages_scraped = 0\n",
    "        \n",
    "        # Store for scraped data\n",
    "        self.scraped_data = []\n",
    "\n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Check if URL is valid and should be scraped\"\"\"\n",
    "        if not url or not url.startswith('http'):\n",
    "            return False\n",
    "            \n",
    "        # Check if it's a Unity-allowed domain\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        \n",
    "        # Check if domain is in allowed list\n",
    "        domain_allowed = any(allowed_domain in domain for allowed_domain in self.allowed_domains)\n",
    "        \n",
    "        # Skip URLs with parameters to avoid duplicate content\n",
    "        has_few_params = len(parsed_url.query) < 30\n",
    "        \n",
    "        # Skip certain file types\n",
    "        skip_extensions = ['.pdf', '.zip', '.exe', '.dmg', '.pkg', '.unitypackage', \n",
    "                          '.jpg', '.jpeg', '.png', '.gif', '.svg', '.mp4', '.webm']\n",
    "        has_valid_extension = not any(url.endswith(ext) for ext in skip_extensions)\n",
    "        \n",
    "        # Skip RSS feeds and other non-content pages\n",
    "        skip_patterns = ['rss', 'feed', 'sitemap', 'login', 'register', 'signup', 'signin']\n",
    "        has_no_skip_patterns = not any(pattern in url.lower() for pattern in skip_patterns)\n",
    "        \n",
    "        return domain_allowed and has_few_params and has_valid_extension and has_no_skip_patterns\n",
    "\n",
    "    def is_unity_related(self, text):\n",
    "        \"\"\"Check if the content is related to Unity\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        return any(re.search(pattern, text_lower) for pattern in self.unity_patterns)\n",
    "\n",
    "    def extract_content(self, soup, url):\n",
    "        \"\"\"\n",
    "        Extract relevant content from a web page\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): Parsed HTML\n",
    "            url (str): URL of the page\n",
    "            \n",
    "        Returns:\n",
    "            dict: Extracted content\n",
    "        \"\"\"\n",
    "        # Extract title\n",
    "        title = soup.title.text.strip() if soup.title else \"No Title\"\n",
    "        \n",
    "        # Extract meta description\n",
    "        meta_desc = \"\"\n",
    "        meta_tag = soup.find(\"meta\", attrs={\"name\": \"description\"}) or soup.find(\"meta\", attrs={\"property\": \"og:description\"})\n",
    "        if meta_tag and meta_tag.get(\"content\"):\n",
    "            meta_desc = meta_tag[\"content\"].strip()\n",
    "            \n",
    "        # Extract main content\n",
    "        content = \"\"\n",
    "        \n",
    "        # Try to find main content area with common selectors\n",
    "        main_selectors = [\n",
    "            \"main\", \"article\", \".content\", \"#content\", \".main-content\", \n",
    "            \".documentation\", \".doc-content\", \".tutorial-content\", \".post-content\",\n",
    "            \".entry-content\", \"#main-content\", \".unity-content\", \".manual-content\"\n",
    "        ]\n",
    "        \n",
    "        for selector in main_selectors:\n",
    "            main_content = soup.select_one(selector)\n",
    "            if main_content:\n",
    "                content += main_content.get_text(separator=\" \", strip=True) + \" \"\n",
    "                break\n",
    "        \n",
    "        # If no main content area found, use <p> tags as fallback\n",
    "        if not content:\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \" \".join(p.get_text(strip=True) for p in paragraphs)\n",
    "        \n",
    "        # Extract code snippets\n",
    "        code_snippets = []\n",
    "        for code_tag in soup.find_all([\"code\", \"pre\"]):\n",
    "            snippet = code_tag.get_text(strip=True)\n",
    "            if snippet and len(snippet) > 10:  # Ignore very short snippets\n",
    "                code_snippets.append(snippet)\n",
    "        \n",
    "        # Extract headings for structure\n",
    "        headings = []\n",
    "        for heading in soup.find_all([\"h1\", \"h2\", \"h3\"]):\n",
    "            heading_text = heading.get_text(strip=True)\n",
    "            if heading_text:\n",
    "                headings.append({\n",
    "                    \"level\": int(heading.name[1]),\n",
    "                    \"text\": heading_text\n",
    "                })\n",
    "        \n",
    "        # Get categories/tags if available\n",
    "        categories = []\n",
    "        for tag in soup.find_all([\"a\", \"span\"], class_=[\"tag\", \"category\", \"topic\"]):\n",
    "            tag_text = tag.get_text(strip=True)\n",
    "            if tag_text:\n",
    "                categories.append(tag_text)\n",
    "        \n",
    "        # Extract timestamp if available\n",
    "        published_date = None\n",
    "        date_selectors = [\n",
    "            'time', '.date', '.published', '.post-date', \n",
    "            'meta[property=\"article:published_time\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in date_selectors:\n",
    "            date_element = soup.select_one(selector)\n",
    "            if date_element:\n",
    "                if date_element.has_attr('datetime'):\n",
    "                    published_date = date_element['datetime']\n",
    "                else:\n",
    "                    published_date = date_element.get_text(strip=True)\n",
    "                break\n",
    "        \n",
    "        # Construct the result\n",
    "        result = {\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"description\": meta_desc,\n",
    "            \"content\": content,\n",
    "            \"headings\": headings,\n",
    "            \"code_snippets\": code_snippets,\n",
    "            \"categories\": categories,\n",
    "            \"published_date\": published_date,\n",
    "            \"scrape_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def extract_links(self, soup, base_url):\n",
    "        \"\"\"\n",
    "        Extract links from a web page\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): Parsed HTML\n",
    "            base_url (str): Base URL for resolving relative links\n",
    "            \n",
    "        Returns:\n",
    "            list: List of extracted links\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        \n",
    "        for a_tag in soup.find_all(\"a\", href=True):\n",
    "            href = a_tag[\"href\"]\n",
    "            \n",
    "            # Skip empty links, javascript, and anchors\n",
    "            if not href or href.startswith((\"javascript:\", \"#\", \"mailto:\", \"tel:\")):\n",
    "                continue\n",
    "                \n",
    "            # Resolve relative URLs\n",
    "            absolute_url = urljoin(base_url, href)\n",
    "            \n",
    "            # Check if the URL is valid\n",
    "            if self.is_valid_url(absolute_url) and absolute_url not in self.visited:\n",
    "                links.append(absolute_url)\n",
    "                \n",
    "        return links\n",
    "\n",
    "    def scrape_url(self, url):\n",
    "        \"\"\"\n",
    "        Scrape a single URL\n",
    "        \n",
    "        Args:\n",
    "            url (str): URL to scrape\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (success, data, new_links)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Add a random delay\n",
    "            time.sleep(self.delay + random.uniform(0.1, 0.5))\n",
    "            \n",
    "            # Make the request\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code != 200:\n",
    "                self.logger.warning(f\"Failed to fetch {url}: Status code {response.status_code}\")\n",
    "                return False, None, []\n",
    "                \n",
    "            # Parse the HTML\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Check if content is Unity-related\n",
    "            page_text = soup.get_text()\n",
    "            if not self.is_unity_related(page_text):\n",
    "                self.logger.info(f\"Skipping {url}: Not Unity-related\")\n",
    "                return False, None, []\n",
    "                \n",
    "            # Extract content\n",
    "            content = self.extract_content(soup, url)\n",
    "            \n",
    "            # Extract links\n",
    "            links = self.extract_links(soup, url)\n",
    "            \n",
    "            return True, content, links\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error scraping {url}: {str(e)}\")\n",
    "            return False, None, []\n",
    "\n",
    "    def process_batch(self, batch):\n",
    "        \"\"\"Process a batch of URLs with concurrent requests\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.concurrent) as executor:\n",
    "            future_to_url = {executor.submit(self.scrape_url, url): url for url in batch}\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    success, content, links = future.result()\n",
    "                    \n",
    "                    if success and content:\n",
    "                        results.append((content, links))\n",
    "                        self.visited.add(url)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing {url}: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def save_batch(self, batch_data, batch_num):\n",
    "        \"\"\"Save a batch of scraped data to JSON\"\"\"\n",
    "        filename = os.path.join(self.output_dir, f\"unity_data_batch_{batch_num}.json\")\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(batch_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        self.logger.info(f\"Saved batch {batch_num} with {len(batch_data)} pages to {filename}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the scraper\"\"\"\n",
    "        self.logger.info(f\"Starting Unity web scraper with {len(self.start_urls)} seed URLs\")\n",
    "        self.logger.info(f\"Max pages: {self.max_pages}, Delay: {self.delay}s, Concurrent: {self.concurrent}\")\n",
    "        \n",
    "        batch_size = 50  # Number of scraped items per file\n",
    "        current_batch = []\n",
    "        batch_count = 1\n",
    "        \n",
    "        # Use tqdm for progress tracking\n",
    "        pbar = tqdm(total=self.max_pages, desc=\"Scraping Unity docs\")\n",
    "        \n",
    "        while self.url_queue and self.pages_scraped < self.max_pages:\n",
    "            # Get a batch of URLs to process\n",
    "            batch_urls = []\n",
    "            while self.url_queue and len(batch_urls) < self.concurrent:\n",
    "                url = self.url_queue.popleft()\n",
    "                if url not in self.visited:\n",
    "                    batch_urls.append(url)\n",
    "                    self.visited.add(url)\n",
    "            \n",
    "            if not batch_urls:\n",
    "                break\n",
    "                \n",
    "            # Process the batch\n",
    "            batch_results = self.process_batch(batch_urls)\n",
    "            \n",
    "            # Handle results\n",
    "            for content, links in batch_results:\n",
    "                # Add content to current batch\n",
    "                current_batch.append(content)\n",
    "                self.pages_scraped += 1\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Add new links to queue\n",
    "                for link in links:\n",
    "                    if link not in self.visited:\n",
    "                        self.url_queue.append(link)\n",
    "                \n",
    "                # Save batch if it reaches batch_size\n",
    "                if len(current_batch) >= batch_size:\n",
    "                    self.save_batch(current_batch, batch_count)\n",
    "                    current_batch = []\n",
    "                    batch_count += 1\n",
    "                    \n",
    "                # Check if we've reached the maximum\n",
    "                if self.pages_scraped >= self.max_pages:\n",
    "                    break\n",
    "        \n",
    "        # Save any remaining data\n",
    "        if current_batch:\n",
    "            self.save_batch(current_batch, batch_count)\n",
    "            \n",
    "        pbar.close()\n",
    "        self.logger.info(f\"Scraping complete. Scraped {self.pages_scraped} pages.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf6c265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 20:50:35,220 - INFO - Starting Unity web scraper with 6 seed URLs\n",
      "2025-04-23 20:50:35,227 - INFO - Max pages: 1000, Delay: 1.0s, Concurrent: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 41\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Create and run the scraper\u001b[39;00m\n\u001b[0;32m     33\u001b[0m scraper \u001b[38;5;241m=\u001b[39m UnityWebScraper(\n\u001b[0;32m     34\u001b[0m     start_urls\u001b[38;5;241m=\u001b[39mseeds,\n\u001b[0;32m     35\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39moutput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     concurrent\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mconcurrent\n\u001b[0;32m     39\u001b[0m )\n\u001b[1;32m---> 41\u001b[0m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 334\u001b[0m, in \u001b[0;36mUnityWebScraper.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# Process the batch\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m batch_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_urls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# Handle results\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m content, links \u001b[38;5;129;01min\u001b[39;00m batch_results:\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;66;03m# Add content to current batch\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 287\u001b[0m, in \u001b[0;36mUnityWebScraper.process_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcurrent) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m    285\u001b[0m     future_to_url \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscrape_url, url): url \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m batch}\n\u001b[1;32m--> 287\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture_to_url\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfuture_to_url\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:243\u001b[0m, in \u001b[0;36mas_completed\u001b[1;34m(fs, timeout)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[0;32m    240\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    241\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[1;32m--> 243\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n\u001b[0;32m    246\u001b[0m     finished \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39mfinished_futures\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Scrape Unity documentation and tutorials')\n",
    "parser.add_argument('--output', '-o', default='./unity_json_data', help='Output directory')\n",
    "parser.add_argument('--max-pages', '-m', type=int, default=1000, help='Maximum number of pages to scrape')\n",
    "parser.add_argument('--delay', '-d', type=float, default=1.0, help='Delay between requests in seconds')\n",
    "parser.add_argument('--concurrent', '-c', type=int, default=5, help='Number of concurrent requests')\n",
    "parser.add_argument('--seed-file', '-s', help='File containing seed URLs (one per line)')\n",
    "args = parser.parse_args(args=[]) \n",
    "\n",
    "default_seeds = [\n",
    "    'https://docs.unity3d.com/Manual/index.html',\n",
    "    'https://learn.unity.com/',\n",
    "    'https://docs.unity3d.com/ScriptReference/index.html',\n",
    "    'https://unity.com/how-to',\n",
    "    'https://forum.unity.com/',\n",
    "    'https://gamedev.stackexchange.com/questions/tagged/unity',\n",
    "]\n",
    "    \n",
    "    # Load seed URLs from file if provided\n",
    "if args.seed_file:\n",
    "    try:\n",
    "        with open(args.seed_file, 'r') as f:\n",
    "            seeds = [line.strip() for line in f if line.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading seed file: {str(e)}\")\n",
    "        print(\"Using default seeds instead.\")\n",
    "        seeds = default_seeds\n",
    "else:\n",
    "    seeds = default_seeds\n",
    "    \n",
    "    # Create and run the scraper\n",
    "scraper = UnityWebScraper(\n",
    "    start_urls=seeds,\n",
    "    output_dir=args.output,\n",
    "    max_pages=args.max_pages,\n",
    "    delay=args.delay,\n",
    "    concurrent=args.concurrent\n",
    ")\n",
    "    \n",
    "scraper.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7056353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityNLPProcessor:\n",
    "    def __init__(self, input_dir, output_dir):\n",
    "        \"\"\"\n",
    "        Initialize NLP processor for Unity documentation\n",
    "        \n",
    "        Args:\n",
    "            input_dir (str): Directory containing JSON files with Unity documentation\n",
    "            output_dir (str): Directory to save processed data and visualizations\n",
    "        \"\"\"\n",
    "        if not os.path.exists(input_dir):\n",
    "            raise ValueError(f\"Input directory {input_dir} does not exist\")\n",
    "            \n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize stopwords\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.unity_stop_words = {\n",
    "            'unity', 'game', 'object', 'function', 'method', 'script', 'component',\n",
    "            'class', 'public', 'private', 'void', 'return', 'using', 'namespace'\n",
    "        }\n",
    "        self.stop_words.update(self.unity_stop_words)\n",
    "        \n",
    "        # Storage for processed data\n",
    "        self.processed_docs = []\n",
    "        self.all_sentences = []\n",
    "        self.all_tokens = []\n",
    "        self.pos_tags = []\n",
    "        self.word_embeddings = {}\n",
    "        \n",
    "    def load_json_data(self):\n",
    "        \"\"\"Load and combine all JSON files from the input directory\"\"\"\n",
    "        combined_data = []\n",
    "        print(f\"Loading JSON files from {self.input_dir}...\")\n",
    "        for filename in tqdm(os.listdir(self.input_dir)):\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(self.input_dir, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        data = json.load(f)\n",
    "                        if isinstance(data, list):\n",
    "                            combined_data.extend(data)\n",
    "                        else:\n",
    "                            combined_data.append(data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {filename}: {str(e)}\")\n",
    "        print(f\"Loaded {len(combined_data)} JSON documents\")\n",
    "        return combined_data\n",
    "    \n",
    "    def extract_text_from_json(self, json_data):\n",
    "        \"\"\"Extract text from JSON data based on common fields\"\"\"\n",
    "        texts = []\n",
    "        for item in json_data:\n",
    "            if isinstance(item, dict):\n",
    "                doc_text = []\n",
    "                if 'content' in item and isinstance(item['content'], str):\n",
    "                    doc_text.append(item['content'])\n",
    "                if 'description' in item and isinstance(item['description'], str):\n",
    "                    doc_text.append(item['description'])\n",
    "                if 'title' in item and isinstance(item['title'], str):\n",
    "                    doc_text.append(item['title'])\n",
    "                if 'code_snippets' in item and isinstance(item['code_snippets'], list):\n",
    "                    for snippet in item['code_snippets']:\n",
    "                        if isinstance(snippet, str):\n",
    "                            doc_text.append(snippet)\n",
    "                if 'headings' in item and isinstance(item['headings'], list):\n",
    "                    for heading in item['headings']:\n",
    "                        if isinstance(heading, dict) and 'text' in heading:\n",
    "                            doc_text.append(heading['text'])\n",
    "                if doc_text:\n",
    "                    texts.append(' '.join(doc_text))\n",
    "        print(f\"Extracted text from {len(texts)} documents\")\n",
    "        return texts\n",
    "    \n",
    "    def preprocess_text(self, texts):\n",
    "        \"\"\"Preprocess extracted text: cleaning and sentence tokenization\"\"\"\n",
    "        processed_docs = []\n",
    "        all_sentences = []\n",
    "        print(\"Preprocessing text...\")\n",
    "        for text in tqdm(texts):\n",
    "            text = re.sub(r'```.*?```', ' ', text, flags=re.DOTALL)\n",
    "            text = re.sub(r'<code>.*?</code>', ' ', text, flags=re.DOTALL)\n",
    "            text = re.sub(r'<[^>]+>', ' ', text)\n",
    "            text = re.sub(r'[^\\w\\s\\.\\(\\)\\[\\]\\{\\}\\<\\>\\+\\-\\*\\/\\=\\:\\;\\,\\&\\|\\!\\?]', ' ', text)\n",
    "            text = text.lower()\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            processed_docs.append(text)\n",
    "            sentences = sent_tokenize(text)\n",
    "            all_sentences.append(sentences)\n",
    "            self.all_sentences.extend(sentences)\n",
    "        self.processed_docs = processed_docs\n",
    "        print(f\"Preprocessed {len(processed_docs)} documents into {sum(len(s) for s in all_sentences)} sentences\")\n",
    "        return processed_docs, all_sentences\n",
    "    \n",
    "    def tokenize_and_remove_stopwords(self):\n",
    "        \"\"\"Tokenize sentences and remove stopwords\"\"\"\n",
    "        tokenized_sentences = []\n",
    "        print(\"Tokenizing and removing stopwords...\")\n",
    "        for sentence in tqdm(self.all_sentences):\n",
    "            tokens = word_tokenize(sentence)\n",
    "            filtered = [t for t in tokens if t.lower() not in self.stop_words and len(t) > 2]\n",
    "            if filtered:\n",
    "                tokenized_sentences.append(filtered)\n",
    "                self.all_tokens.extend(filtered)\n",
    "        print(f\"Generated {len(tokenized_sentences)} tokenized sentences\")\n",
    "        return tokenized_sentences\n",
    "    \n",
    "    def perform_pos_tagging(self):\n",
    "        \"\"\"Perform POS tagging using spaCy\"\"\"\n",
    "        print(\"Performing POS tagging...\")\n",
    "        pos_tags = []\n",
    "        for doc in tqdm(nlp.pipe(self.all_sentences, batch_size=50, disable=[\"ner\"])):\n",
    "            pos_tags.extend([(token.text, token.pos_) for token in doc])\n",
    "        self.pos_tags = pos_tags\n",
    "        pos_counts = Counter([tag for _, tag in pos_tags])\n",
    "        with open(os.path.join(self.output_dir, 'pos_distribution.json'), 'w') as f:\n",
    "            json.dump(pos_counts, f, indent=2)\n",
    "        print(f\"Performed POS tagging on {len(pos_tags)} tokens\")\n",
    "        return pos_tags\n",
    "    \n",
    "    def analyze_pos_distribution(self):\n",
    "        \"\"\"Analyze and visualize POS distribution\"\"\"\n",
    "        print(\"Analyzing POS distribution...\")\n",
    "        pos_counts = Counter([tag for _, tag in self.pos_tags])\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        labels, values = zip(*sorted(pos_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "        plt.bar(labels, values)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'pos_distribution.png'))\n",
    "        nouns = [w for w, t in self.pos_tags if t=='NOUN']\n",
    "        verbs = [w for w, t in self.pos_tags if t=='VERB']\n",
    "        adjs = [w for w, t in self.pos_tags if t=='ADJ']\n",
    "        with open(os.path.join(self.output_dir, 'common_words_by_pos.json'), 'w') as f:\n",
    "            json.dump({\n",
    "                'nouns': dict(Counter(nouns).most_common(30)),\n",
    "                'verbs': dict(Counter(verbs).most_common(30)),\n",
    "                'adjectives': dict(Counter(adjs).most_common(30))\n",
    "            }, f, indent=2)\n",
    "        return {'nouns': Counter(nouns).most_common(30), 'verbs': Counter(verbs).most_common(30), 'adjectives': Counter(adjs).most_common(30)}\n",
    "    \n",
    "    def train_word2vec(self, tokenized_sentences):\n",
    "        \"\"\"Train a Word2Vec model\"\"\"\n",
    "        print(\"Training Word2Vec model...\")\n",
    "        if not tokenized_sentences:\n",
    "            print(\"No tokenized sentences for Word2Vec\")\n",
    "            return None\n",
    "        model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=5, workers=4, sg=1, epochs=10)\n",
    "        model.save(os.path.join(self.output_dir, 'unity_word2vec.model'))\n",
    "        print(f\"Word2Vec model trained with {len(model.wv)} words\")\n",
    "        return model\n",
    "    \n",
    "    def visualize_embeddings(self, model, n_words=100):\n",
    "        \"\"\"Visualize embeddings with t-SNE\"\"\"\n",
    "        print(\"Visualizing word embeddings...\")\n",
    "        common = [w for w, _ in Counter(self.all_tokens).most_common(n_words) if w in model.wv]\n",
    "        if len(common) < 2:\n",
    "            print(\"Not enough words for t-SNE\")\n",
    "            return None\n",
    "        vectors = np.array([model.wv[w] for w in common])\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(common)-1))\n",
    "        coords = tsne.fit_transform(vectors)\n",
    "        df = pd.DataFrame({'word': common, 'x': coords[:,0], 'y': coords[:,1]})\n",
    "        plt.figure(figsize=(16,10))\n",
    "        sns.scatterplot(data=df, x='x', y='y')\n",
    "        for i, row in df.iterrows():\n",
    "            plt.annotate(row['word'], (row['x'], row['y']), fontsize=8)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'word_embeddings_tsne.png'))\n",
    "        return df\n",
    "    \n",
    "    def find_similar_words(self, model, words, n=10):\n",
    "        \"\"\"Find similar words using Word2Vec\"\"\"\n",
    "        print(\"Finding similar words...\")\n",
    "        results = {}\n",
    "        for w in words:\n",
    "            if w in model.wv:\n",
    "                results[w] = model.wv.most_similar(w, topn=n)\n",
    "        with open(os.path.join(self.output_dir, 'similar_words.json'), 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        return results\n",
    "\n",
    "    def process_pipeline(self):\n",
    "        \"\"\"Run full NLP pipeline and return processed dataset\"\"\"\n",
    "        # Load and extract\n",
    "        json_data = self.load_json_data()\n",
    "        texts = self.extract_text_from_json(json_data)\n",
    "        if not texts:\n",
    "            print(\"No text extracted. Aborting.\")\n",
    "            return None\n",
    "        # Preprocess\n",
    "        processed_docs, _ = self.preprocess_text(texts)\n",
    "        # Tokenize & POS\n",
    "        tokenized = self.tokenize_and_remove_stopwords()\n",
    "        self.perform_pos_tagging()\n",
    "        self.analyze_pos_distribution()\n",
    "        # Word2Vec\n",
    "        model = None\n",
    "        if tokenized:\n",
    "            model = self.train_word2vec(tokenized)\n",
    "            if model:\n",
    "                self.visualize_embeddings(model)\n",
    "                self.find_similar_words(model, [\n",
    "                    'gameobject','transform','component','rigidbody','collider',\n",
    "                    'vector','quaternion','material','texture','mesh','animation',\n",
    "                    'script','monobehaviour','instantiate','destroy','update'\n",
    "                ])\n",
    "        print(\"NLP processing pipeline complete\")\n",
    "        # Create a DataFrame of processed documents\n",
    "        df = pd.DataFrame({'processed_text': self.processed_docs})\n",
    "        # Save processed dataset\n",
    "        output_csv = os.path.join(self.output_dir, 'processed_dataset.csv')\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Processed dataset saved to {output_csv}\")\n",
    "        return df\n",
    "\n",
    "# Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f04f6ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSON files from ./unity_json_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 264.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 JSON documents\n",
      "Extracted text from 1000 documents\n",
      "Preprocessing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 988.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 1000 documents into 15564 sentences\n",
      "Tokenizing and removing stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 15564/15564 [00:03<00:00, 4244.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 15528 tokenized sentences\n",
      "Performing POS tagging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./unity_nlp_results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m processor \u001b[38;5;241m=\u001b[39m UnityNLPProcessor(input_dir, output_dir)\n\u001b[1;32m----> 5\u001b[0m processed_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_df, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(processed_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[17], line 198\u001b[0m, in \u001b[0;36mUnityNLPProcessor.process_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# Tokenize & POS\u001b[39;00m\n\u001b[0;32m    197\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_and_remove_stopwords()\n\u001b[1;32m--> 198\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_pos_tagging\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyze_pos_distribution()\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# Word2Vec\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 115\u001b[0m, in \u001b[0;36mUnityNLPProcessor.perform_pos_tagging\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforming POS tagging...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m pos_tags \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mnlp\u001b[49m\u001b[38;5;241m.\u001b[39mpipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_sentences, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, disable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m])):\n\u001b[0;32m    116\u001b[0m     pos_tags\u001b[38;5;241m.\u001b[39mextend([(token\u001b[38;5;241m.\u001b[39mtext, token\u001b[38;5;241m.\u001b[39mpos_) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc])\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_tags \u001b[38;5;241m=\u001b[39m pos_tags\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"./unity_json_data\"\n",
    "    output_dir = \"./unity_nlp_results\"\n",
    "    processor = UnityNLPProcessor(input_dir, output_dir)\n",
    "    processed_df = processor.process_pipeline()\n",
    "    if isinstance(processed_df, pd.DataFrame):\n",
    "        print(processed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2c2c0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\shash\\AppData\\Local\\Temp\\ipykernel_1588\\4070161523.py:5: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  df = pd.read_csv(\"unity_nlp_results\\processed_dataset.csv\")  # ← adjust filename/path as needed\n",
      "C:\\Users\\shash\\AppData\\Local\\Temp\\ipykernel_1588\\4070161523.py:21: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  df.to_csv(\"unity_nlp_results\\processed_dataset.csv\", index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset written to results/your_results_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load your processed dataset\n",
    "df = pd.read_csv(\"unity_nlp_results\\processed_dataset.csv\")  # ← adjust filename/path as needed\n",
    "\n",
    "# 2. Function to remove non-ASCII (and non-English) chars\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    # drop anything outside the basic ASCII range:\n",
    "    ascii_only = s.encode(\"ascii\", errors=\"ignore\").decode(\"ascii\")\n",
    "    # optionally further restrict to letters/numbers/punctuation/whitespace:\n",
    "    return re.sub(r\"[^A-Za-z0-9\\s\\!\\?\\,\\.\\-\\'\\\"]+\", \"\", ascii_only)\n",
    "\n",
    "# 3. Apply to every object (string) column\n",
    "for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    df[col] = df[col].apply(clean_text)\n",
    "\n",
    "# 4. Save out a cleaned CSV\n",
    "df.to_csv(\"unity_nlp_results\\processed_dataset.csv\", index=False)\n",
    "print(f\"Cleaned dataset written to results/your_results_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f77def25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "191a4afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbf07d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client =chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(\"unity_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eabbf2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  print(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = torch.device(\"mps\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be57fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model_name=\"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f95ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a703f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"unity_nlp_results/processed_dataset.csv\", encoding=\"utf-8\")\n",
    "texts = df[\"processed_text\"].astype(str).tolist()\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(texts)\n",
    "embeddings = np.array(embeddings, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f7caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = texts \n",
    "ids = [f\"doc_{i}\" for i in range(len(documents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061aac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    ids=ids,\n",
    "    documents=documents,\n",
    "    embeddings=embeddings  # this stays as float32 vectors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85872b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "manual scripting api unity.com unity - manual introduction to scenes unity manual introduction to scenes scene templates the new scene dialog creating a new scene pinning templates locating and editing templates multi-scene editing\n",
      "\n",
      "Result 2:\n",
      "enhance your unity projects with our guide on 11 programming patterns. download the sample project and start coding better today! level up your code with design patterns and solid e-book  unity\n",
      "\n",
      "Result 3:\n",
      "this post documents how to use the features of the post composer and markdown used by unity discussions. use the table of contents in the top right of the post to navigate the sections. this document is not exhaus powered bydiscourse, best viewed with javascript enabled ask questions, provide feedback, or discuss unity s web resources, including the unity asset store, unity discussions, unity documentation, unity learn, and unity-play. web resources - unity discussions web resources\n",
      "\n",
      "Result 4:\n",
      "this post documents how to use the features of the post composer and markdown used by unity discussions. use the table of contents in the top right of the post to navigate the sections. this document is not exhaus powered bydiscourse, best viewed with javascript enabled ask questions, provide feedback, or discuss unity s web resources, including the unity asset store, unity discussions, unity documentation, unity learn, and unity-play. web resources - unity discussions web resources\n",
      "\n",
      "Result 5:\n",
      "this post documents how to use the features of the post composer and markdown used by unity discussions. use the table of contents in the top right of the post to navigate the sections. this document is not exhaus powered bydiscourse, best viewed with javascript enabled ask questions, provide feedback, or discuss unity s web resources, including the unity asset store, unity discussions, unity documentation, unity learn, and unity-play. web resources - unity discussions web resources\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"template\"],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(results['documents'][0]):\n",
    "    print(f\"\\nResult {i+1}:\\n{doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "882ae980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define ChromaDB retrieval functions\n",
    "def retrieve_docs_chromadb(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieve documents from ChromaDB based on the query.\n",
    "    \"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    retrieved_docs = results['documents'][0] if 'documents' in results and results['documents'] else []\n",
    "    print(\"\\n🔍 Retrieved Documents:\\n\", retrieved_docs)\n",
    "    return retrieved_docs\n",
    "\n",
    "# 2. Define RAG-based answer generation using LLaMA\n",
    "def generate_answer(query):\n",
    "    \"\"\"\n",
    "    Generate an answer using the LLaMA model and retrieved documents.\n",
    "    \"\"\"\n",
    "    retrieved_docs = retrieve_docs_chromadb(query, top_k=3)\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        return \"No relevant information found.\"\n",
    "\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant answering questions about Unity's documentation.\n",
    "    Context:\n",
    "    {context}\n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, max_new_tokens=150)\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e01dae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7092182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://027440a44a0d114c67.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://027440a44a0d114c67.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Retrieved Documents:\n",
      " ['manual scripting api unity.com unity - manual introduction to collision unity manual introduction to collision collider types trigger colliders collider shapes collider surfaces', 'manual scripting api unity.com unity - manual introduction to rigidbody 2d unity manual introduction to rigidbody 2d how a rigidbody 2d works collider 2d and rigidbody 2d interaction additional resources', 'manual scripting api unity.com unity - manual physics unity manual physics built-in physics engines for object-oriented projects physics engine packages for data-oriented projects additional information resources', 'hi everyone, we just created an updated 7-part video tutorial series on the input system full of tips on how to make the most of various use cases. we cover everything you ll need to get up and running with the input sy hey everyone, a new e-book just dropped for those of you using or planning to use urp in your projects. read on to get key details about the e-book, its companion collection of sample scenes, and a couple of great you hi everyone smiley, we re very happy to announce the availability of the unity 6 version of the deep-dive technical e-book the universal render pipeline for advanced unity creators.  download the unity 6 edition powered bydiscourse, best viewed with javascript enabled welcome to the technical articles category. technical articles - unity discussions technical articles', 'hi everyone, we just created an updated 7-part video tutorial series on the input system full of tips on how to make the most of various use cases. we cover everything you ll need to get up and running with the input sy hey everyone, a new e-book just dropped for those of you using or planning to use urp in your projects. read on to get key details about the e-book, its companion collection of sample scenes, and a couple of great you hi everyone smiley, we re very happy to announce the availability of the unity 6 version of the deep-dive technical e-book the universal render pipeline for advanced unity creators.  download the unity 6 edition powered bydiscourse, best viewed with javascript enabled welcome to the technical articles category. technical articles - unity discussions technical articles']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Retrieved Documents:\n",
      " ['manual scripting api unity.com unity - manual introduction to collision unity manual introduction to collision collider types trigger colliders collider shapes collider surfaces', 'manual scripting api unity.com unity - manual introduction to rigidbody 2d unity manual introduction to rigidbody 2d how a rigidbody 2d works collider 2d and rigidbody 2d interaction additional resources', 'manual scripting api unity.com unity - manual physics unity manual physics built-in physics engines for object-oriented projects physics engine packages for data-oriented projects additional information resources', 'hi everyone, we just created an updated 7-part video tutorial series on the input system full of tips on how to make the most of various use cases. we cover everything you ll need to get up and running with the input sy hey everyone, a new e-book just dropped for those of you using or planning to use urp in your projects. read on to get key details about the e-book, its companion collection of sample scenes, and a couple of great you hi everyone smiley, we re very happy to announce the availability of the unity 6 version of the deep-dive technical e-book the universal render pipeline for advanced unity creators.  download the unity 6 edition powered bydiscourse, best viewed with javascript enabled welcome to the technical articles category. technical articles - unity discussions technical articles', 'hi everyone, we just created an updated 7-part video tutorial series on the input system full of tips on how to make the most of various use cases. we cover everything you ll need to get up and running with the input sy hey everyone, a new e-book just dropped for those of you using or planning to use urp in your projects. read on to get key details about the e-book, its companion collection of sample scenes, and a couple of great you hi everyone smiley, we re very happy to announce the availability of the unity 6 version of the deep-dive technical e-book the universal render pipeline for advanced unity creators.  download the unity 6 edition powered bydiscourse, best viewed with javascript enabled welcome to the technical articles category. technical articles - unity discussions technical articles']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Define CoT (Chain-of-Thought) reasoning\n",
    "def generate_cot_answer(query):\n",
    "    \"\"\"\n",
    "    Generate an answer using Chain-of-Thought reasoning.\n",
    "    \"\"\"\n",
    "    retrieved_docs = retrieve_docs_chromadb(query, top_k=3)\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        return \"No relevant information found.\"\n",
    "\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    cot_prompt = f\"\"\"\n",
    "    You are an expert in Unity game development. Break down the reasoning step-by-step to answer the following query:\n",
    "    Context:\n",
    "    {context}\n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(cot_prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, max_new_tokens=300)\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Define ToT (Tree-of-Thought) reasoning\n",
    "def generate_tot_answer(query, depth=2):\n",
    "    \"\"\"\n",
    "    Generate an answer using Tree-of-Thought reasoning.\n",
    "    \"\"\"\n",
    "    def recursive_reasoning(subquery, current_depth):\n",
    "        if current_depth > depth:\n",
    "            return []\n",
    "\n",
    "        retrieved_docs = retrieve_docs_chromadb(subquery, top_k=3)\n",
    "        if not retrieved_docs:\n",
    "            return [f\"No relevant information found for: {subquery}\"]\n",
    "\n",
    "        context = \"\\n\".join(retrieved_docs)\n",
    "        tot_prompt = f\"\"\"\n",
    "        You are an expert in Unity game development. Explore multiple reasoning paths to answer the following query:\n",
    "        Context:\n",
    "        {context}\n",
    "        Question: {subquery}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = tokenizer(tot_prompt, return_tensors=\"pt\").to(device)\n",
    "        output = model.generate(**inputs, max_new_tokens=300)\n",
    "        answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Recursively explore sub-questions\n",
    "        subquestions = [f\"Sub-question {i+1}: {doc}\" for i, doc in enumerate(retrieved_docs)]\n",
    "        subanswers = []\n",
    "        for subq in subquestions:\n",
    "            subanswers.extend(recursive_reasoning(subq, current_depth + 1))\n",
    "\n",
    "        return [answer] + subanswers\n",
    "\n",
    "    return \"\\n\".join(recursive_reasoning(query, 1))\n",
    "\n",
    "# Define GoT (Graph-of-Thought) reasoning\n",
    "def generate_got_answer(query):\n",
    "    \"\"\"\n",
    "    Generate an answer using Graph-of-Thought reasoning.\n",
    "    \"\"\"\n",
    "    retrieved_docs = retrieve_docs_chromadb(query, top_k=5)\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        return \"No relevant information found.\"\n",
    "\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    got_prompt = f\"\"\"\n",
    "    You are an expert in Unity game development. Explore interconnected ideas and relationships to answer the following query:\n",
    "    Context:\n",
    "    {context}\n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(got_prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, max_new_tokens=500)\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Extend Gradio interface to include CoT, ToT, and GoT\n",
    "def chat_interface_with_reasoning(user_input, reasoning_type=\"CoT\"):\n",
    "    \"\"\"\n",
    "    Chat interface function with reasoning options.\n",
    "    \"\"\"\n",
    "    if reasoning_type == \"CoT\":\n",
    "        return generate_cot_answer(user_input)\n",
    "    elif reasoning_type == \"ToT\":\n",
    "        return generate_tot_answer(user_input)\n",
    "    elif reasoning_type == \"GoT\":\n",
    "        return generate_got_answer(user_input)\n",
    "    else:\n",
    "        return \"Invalid reasoning type. Please choose CoT, ToT, or GoT.\"\n",
    "\n",
    "gr.Interface(\n",
    "    fn=chat_interface_with_reasoning,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Ask your Unity question\"),\n",
    "        gr.Radio(choices=[\"CoT\", \"ToT\", \"GoT\"], label=\"Reasoning Type\", value=\"CoT\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"AI Response\"),\n",
    "    title=\"🎮 Unity Assistant Chatbot with Reasoning\",\n",
    "    description=\"Ask anything about Unity development or documentation! Choose a reasoning type: CoT, ToT, or GoT.\",\n",
    "    theme=\"default\"\n",
    ").launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
